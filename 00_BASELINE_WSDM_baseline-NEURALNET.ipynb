{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Der nachfolgende Programmcode kann erst vollständig ausgeführt werden, wenn  die Datenaufbereitung für alle Verfahren\n",
    "#  erfolgt ist. Hierzu müssen zuerst die weiteren als Jupyter Notebook zur Verfügung gestellten Programme vollständig\n",
    "#  ausgeführt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1643537698691,
     "user": {
      "displayName": "Markus",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08156928817414143810"
     },
     "user_tz": -60
    },
    "id": "59Atc0GfduwM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import gc; gc.enable()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Modelling - \n",
    "    SUM MIN MAX (ohne Rohdaten)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://machinelearningmastery.com/how-to-save-a-numpy-array-to-file-for-machine-learning/\n",
    "X_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_train.npz\")\n",
    "y_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_wsdm_labels.npz\")\n",
    "X_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_cv.npz\")\n",
    "y_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_cv_labels.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314428, 37) (314428,)\n",
      "(134756, 37) (134756,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train['arr_0']\n",
    "X_test = X_test['arr_0']\n",
    "y_train = y_train['arr_0']\n",
    "y_test = y_test['arr_0']\n",
    "\n",
    "# getting shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7163838283277888506]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting device information\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 37)]              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,281\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "init = tf.keras.initializers.HeUniform()\n",
    "regular = tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(37,))\n",
    "\n",
    "# Dense hidden layer 1\n",
    "layer1 = Dense(64, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(input_layer)\n",
    "\n",
    "# Batch Normalization layer 1\n",
    "b_norm1 = BatchNormalization()(layer1)\n",
    "\n",
    "# Dropout layer 1\n",
    "drop1 = Dropout(rate=0.5)(b_norm1)\n",
    "\n",
    "# Dense hidden layer 2\n",
    "layer2 = Dense(32, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop1)\n",
    "\n",
    "# Batch Normalization layer 2\n",
    "b_norm2 = BatchNormalization()(layer2)\n",
    "\n",
    "# Dropout layer 2\n",
    "drop2 = Dropout(rate=0.5)(b_norm2)\n",
    "\n",
    "# Dense hidden layer 3\n",
    "layer3 = Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop2)\n",
    "\n",
    "# Batch Normalization layer 3\n",
    "b_norm3 = BatchNormalization()(layer3)\n",
    "\n",
    "# Dropout layer 3\n",
    "drop3 = Dropout(rate=0.5)(b_norm3)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(), kernel_regularizer=regular)(drop3)\n",
    "\n",
    "# Creating a model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "## Verwende Loss Funktion\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['Accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "cb = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1229/1229 [==============================] - 5s 3ms/step - loss: 0.7622 - Accuracy: 0.8799 - recall_1: 0.4330 - precision_1: 0.3600 - val_loss: 0.2246 - val_Accuracy: 0.9387 - val_recall_1: 0.4451 - val_precision_1: 0.8021\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22464, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\n",
      "Epoch 2/10\n",
      "1229/1229 [==============================] - 4s 3ms/step - loss: 0.2121 - Accuracy: 0.9375 - recall_1: 0.4823 - precision_1: 0.7304 - val_loss: 0.2361 - val_Accuracy: 0.9420 - val_recall_1: 0.6632 - val_precision_1: 0.6945\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.22464\n",
      "Epoch 3/10\n",
      "1229/1229 [==============================] - 4s 3ms/step - loss: 0.1907 - Accuracy: 0.9398 - recall_1: 0.4987 - precision_1: 0.7471 - val_loss: 0.1739 - val_Accuracy: 0.9453 - val_recall_1: 0.5479 - val_precision_1: 0.7952\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22464 to 0.17394, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\n",
      "Epoch 4/10\n",
      "1229/1229 [==============================] - 4s 3ms/step - loss: 0.1848 - Accuracy: 0.9397 - recall_1: 0.4984 - precision_1: 0.7453 - val_loss: 0.1694 - val_Accuracy: 0.9468 - val_recall_1: 0.5785 - val_precision_1: 0.7884\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17394 to 0.16938, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\n",
      "Epoch 5/10\n",
      "1229/1229 [==============================] - 4s 3ms/step - loss: 0.1826 - Accuracy: 0.9403 - recall_1: 0.4994 - precision_1: 0.7521 - val_loss: 0.1688 - val_Accuracy: 0.9423 - val_recall_1: 0.5138 - val_precision_1: 0.7863\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16938 to 0.16880, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\n",
      "Epoch 6/10\n",
      "1229/1229 [==============================] - 4s 3ms/step - loss: 0.1810 - Accuracy: 0.9405 - recall_1: 0.5056 - precision_1: 0.7501 - val_loss: 0.1784 - val_Accuracy: 0.9414 - val_recall_1: 0.4990 - val_precision_1: 0.7873\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16880\n",
      "Epoch 7/10\n",
      "1229/1229 [==============================] - 4s 3ms/step - loss: 0.1788 - Accuracy: 0.9414 - recall_1: 0.5133 - precision_1: 0.7558 - val_loss: 0.1846 - val_Accuracy: 0.9423 - val_recall_1: 0.4878 - val_precision_1: 0.8114\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16880\n",
      "Epoch 8/10\n",
      "1229/1229 [==============================] - 4s 3ms/step - loss: 0.1788 - Accuracy: 0.9410 - recall_1: 0.5070 - precision_1: 0.7552 - val_loss: 0.1975 - val_Accuracy: 0.9419 - val_recall_1: 0.5437 - val_precision_1: 0.7584\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16880\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2. Modelling - \n",
    "    SUM MIN MAX (inkl. Rohdaten)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://machinelearningmastery.com/how-to-save-a-numpy-array-to-file-for-machine-learning/\n",
    "X_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_train_incraw.npz\")\n",
    "y_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_wsdm_labels_incraw.npz\")\n",
    "X_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_cv_incraw.npz\")\n",
    "y_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\actual_cv_labels_incraw.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4137529, 52) (4137529,)\n",
      "(1773228, 52) (1773228,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train['arr_0']\n",
    "X_test = X_test['arr_0']\n",
    "y_train = y_train['arr_0']\n",
    "y_test = y_test['arr_0']\n",
    "\n",
    "# getting shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11663723608212709350]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting device information\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 52)]              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 64)                3392      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,465\n",
      "Trainable params: 6,241\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "init = tf.keras.initializers.HeUniform()\n",
    "regular = tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(52,))\n",
    "\n",
    "# Dense hidden layer 1\n",
    "layer1 = Dense(64, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(input_layer)\n",
    "\n",
    "# Batch Normalization layer 1\n",
    "b_norm1 = BatchNormalization()(layer1)\n",
    "\n",
    "# Dropout layer 1\n",
    "drop1 = Dropout(rate=0.5)(b_norm1)\n",
    "\n",
    "# Dense hidden layer 2\n",
    "layer2 = Dense(32, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop1)\n",
    "\n",
    "# Batch Normalization layer 2\n",
    "b_norm2 = BatchNormalization()(layer2)\n",
    "\n",
    "# Dropout layer 2\n",
    "drop2 = Dropout(rate=0.5)(b_norm2)\n",
    "\n",
    "# Dense hidden layer 3\n",
    "layer3 = Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop2)\n",
    "\n",
    "# Batch Normalization layer 3\n",
    "b_norm3 = BatchNormalization()(layer3)\n",
    "\n",
    "# Dropout layer 3\n",
    "drop3 = Dropout(rate=0.5)(b_norm3)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(), kernel_regularizer=regular)(drop3)\n",
    "\n",
    "# Creating a model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "## Verwende Loss Funktion\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['Accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "cb = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16163/16163 [==============================] - 43s 3ms/step - loss: 0.2816 - Accuracy: 0.9143 - recall_14: 0.3337 - precision_14: 0.6659 - val_loss: 0.2519 - val_Accuracy: 0.9128 - val_recall_14: 0.1609 - val_precision_14: 0.9238\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25188, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\n",
      "Epoch 2/10\n",
      "16163/16163 [==============================] - 40s 2ms/step - loss: 0.2238 - Accuracy: 0.9194 - recall_14: 0.3514 - precision_14: 0.7219 - val_loss: 0.4553 - val_Accuracy: 0.8961 - val_recall_14: 0.5481 - val_precision_14: 0.4932\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25188\n",
      "Epoch 3/10\n",
      "16163/16163 [==============================] - 40s 2ms/step - loss: 0.2233 - Accuracy: 0.9194 - recall_14: 0.3526 - precision_14: 0.7212 - val_loss: 0.2083 - val_Accuracy: 0.9292 - val_recall_14: 0.4031 - val_precision_14: 0.8094\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25188 to 0.20832, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_2\\best_model.h5\n",
      "Epoch 4/10\n",
      "16163/16163 [==============================] - 40s 2ms/step - loss: 0.2235 - Accuracy: 0.9190 - recall_14: 0.3498 - precision_14: 0.7180 - val_loss: 0.5276 - val_Accuracy: 0.8631 - val_recall_14: 0.4429 - val_precision_14: 0.3619\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.20832\n",
      "Epoch 5/10\n",
      "16163/16163 [==============================] - 41s 3ms/step - loss: 0.2235 - Accuracy: 0.9191 - recall_14: 0.3496 - precision_14: 0.7184 - val_loss: 0.3013 - val_Accuracy: 0.9252 - val_recall_14: 0.3910 - val_precision_14: 0.7622\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.20832\n",
      "Epoch 6/10\n",
      "16163/16163 [==============================] - 39s 2ms/step - loss: 0.2237 - Accuracy: 0.9190 - recall_14: 0.3480 - precision_14: 0.7191 - val_loss: 0.4664 - val_Accuracy: 0.8829 - val_recall_14: 0.5824 - val_precision_14: 0.4448\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.20832\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 3. Modelling - \n",
    "    Domain Driven Feature Labeling (ohne Rohdaten)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_train.npz\")\n",
    "y_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_wsdm_labels.npz\")\n",
    "X_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_test.npz\")\n",
    "y_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_test_labels.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(679672, 134) (679672,)\n",
      "(291288, 134) (291288,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train['arr_0']\n",
    "X_test = X_test['arr_0']\n",
    "y_train = y_train['arr_0']\n",
    "y_test = y_test['arr_0']\n",
    "\n",
    "# getting shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16372060046434871542]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting device information\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 134)]             0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 64)                8640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,713\n",
      "Trainable params: 11,489\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "init = tf.keras.initializers.HeUniform()\n",
    "regular = tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(134,))\n",
    "\n",
    "# Dense hidden layer 1\n",
    "layer1 = Dense(64, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(input_layer)\n",
    "\n",
    "# Batch Normalization layer 1\n",
    "b_norm1 = BatchNormalization()(layer1)\n",
    "\n",
    "# Dropout layer 1\n",
    "drop1 = Dropout(rate=0.5)(b_norm1)\n",
    "\n",
    "# Dense hidden layer 2\n",
    "layer2 = Dense(32, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop1)\n",
    "\n",
    "# Batch Normalization layer 2\n",
    "b_norm2 = BatchNormalization()(layer2)\n",
    "\n",
    "# Dropout layer 2\n",
    "drop2 = Dropout(rate=0.5)(b_norm2)\n",
    "\n",
    "# Dense hidden layer 3\n",
    "layer3 = Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop2)\n",
    "\n",
    "# Batch Normalization layer 3\n",
    "b_norm3 = BatchNormalization()(layer3)\n",
    "\n",
    "# Dropout layer 3\n",
    "drop3 = Dropout(rate=0.5)(b_norm3)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(), kernel_regularizer=regular)(drop3)\n",
    "\n",
    "# Creating a model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "## Verwende Loss Funktion\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['Accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "cb = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2655/2655 [==============================] - 9s 3ms/step - loss: 0.4084 - Accuracy: 0.9304 - recall_13: 0.5044 - precision_13: 0.6452 - val_loss: 0.3106 - val_Accuracy: 0.9521 - val_recall_13: 0.5632 - val_precision_13: 0.8518\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31056, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 2/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1791 - Accuracy: 0.9501 - recall_13: 0.5708 - precision_13: 0.8210 - val_loss: 0.3708 - val_Accuracy: 0.9212 - val_recall_13: 0.7491 - val_precision_13: 0.5439\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31056\n",
      "Epoch 3/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1738 - Accuracy: 0.9500 - recall_13: 0.5703 - precision_13: 0.8194 - val_loss: 0.2281 - val_Accuracy: 0.9334 - val_recall_13: 0.2654 - val_precision_13: 0.9681\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31056 to 0.22812, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 4/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1721 - Accuracy: 0.9496 - recall_13: 0.5672 - precision_13: 0.8169 - val_loss: 0.4011 - val_Accuracy: 0.9568 - val_recall_13: 0.6295 - val_precision_13: 0.8502\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.22812\n",
      "Epoch 5/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1744 - Accuracy: 0.9480 - recall_13: 0.5519 - precision_13: 0.8105 - val_loss: 0.2726 - val_Accuracy: 0.9482 - val_recall_13: 0.4888 - val_precision_13: 0.8792\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22812\n",
      "Epoch 6/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1723 - Accuracy: 0.9492 - recall_13: 0.5583 - precision_13: 0.8200 - val_loss: 0.1893 - val_Accuracy: 0.9333 - val_recall_13: 0.2784 - val_precision_13: 0.9240\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.22812 to 0.18932, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 7/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1729 - Accuracy: 0.9490 - recall_13: 0.5570 - precision_13: 0.8189 - val_loss: 0.1821 - val_Accuracy: 0.9585 - val_recall_13: 0.6518 - val_precision_13: 0.8505\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.18932 to 0.18215, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 8/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1711 - Accuracy: 0.9495 - recall_13: 0.5616 - precision_13: 0.8216 - val_loss: 0.1680 - val_Accuracy: 0.9470 - val_recall_13: 0.4981 - val_precision_13: 0.8472\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18215 to 0.16801, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 9/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1732 - Accuracy: 0.9485 - recall_13: 0.5520 - precision_13: 0.8173 - val_loss: 1.0483 - val_Accuracy: 0.2579 - val_recall_13: 0.7578 - val_precision_13: 0.0862\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16801\n",
      "Epoch 10/10\n",
      "2655/2655 [==============================] - 7s 3ms/step - loss: 0.1972 - Accuracy: 0.9416 - recall_13: 0.4672 - precision_13: 0.8024 - val_loss: 0.2717 - val_Accuracy: 0.9276 - val_recall_13: 0.6230 - val_precision_13: 0.5916\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.16801\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 4. Modelling - \n",
    "    Domain Driven Feature Labeling (mit Rohdaten)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_train_incraw.npz\")\n",
    "y_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_wsdm_labels_incraw.npz\")\n",
    "X_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_test_incraw.npz\")\n",
    "y_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\actual_test_labels_incraw.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3310024, 160) (3310024,)\n",
      "(1418582, 160) (1418582,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train['arr_0']\n",
    "X_test = X_test['arr_0']\n",
    "y_train = y_train['arr_0']\n",
    "y_test = y_test['arr_0']\n",
    "\n",
    "# getting shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 235336893697470730]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting device information\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 160)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,377\n",
      "Trainable params: 13,153\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "init = tf.keras.initializers.HeUniform()\n",
    "regular = tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(160,))\n",
    "\n",
    "# Dense hidden layer 1\n",
    "layer1 = Dense(64, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(input_layer)\n",
    "\n",
    "# Batch Normalization layer 1\n",
    "b_norm1 = BatchNormalization()(layer1)\n",
    "\n",
    "# Dropout layer 1\n",
    "drop1 = Dropout(rate=0.5)(b_norm1)\n",
    "\n",
    "# Dense hidden layer 2\n",
    "layer2 = Dense(32, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop1)\n",
    "\n",
    "# Batch Normalization layer 2\n",
    "b_norm2 = BatchNormalization()(layer2)\n",
    "\n",
    "# Dropout layer 2\n",
    "drop2 = Dropout(rate=0.5)(b_norm2)\n",
    "\n",
    "# Dense hidden layer 3\n",
    "layer3 = Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop2)\n",
    "\n",
    "# Batch Normalization layer 3\n",
    "b_norm3 = BatchNormalization()(layer3)\n",
    "\n",
    "# Dropout layer 3\n",
    "drop3 = Dropout(rate=0.5)(b_norm3)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(), kernel_regularizer=regular)(drop3)\n",
    "\n",
    "# Creating a model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "## Verwende Loss Funktion\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['Accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "cb = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12930/12930 [==============================] - 31s 2ms/step - loss: 0.3710 - Accuracy: 0.8937 - recall: 0.0135 - precision: 0.2186 - val_loss: 0.3438 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34376, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 2/10\n",
      "12930/12930 [==============================] - 30s 2ms/step - loss: 0.3203 - Accuracy: 0.8967 - recall: 0.0142 - precision: 0.4244 - val_loss: 0.3709 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.34376\n",
      "Epoch 3/10\n",
      "12930/12930 [==============================] - 29s 2ms/step - loss: 0.3221 - Accuracy: 0.8968 - recall: 0.0057 - precision: 0.3530 - val_loss: 0.3463 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.34376\n",
      "Epoch 4/10\n",
      "12930/12930 [==============================] - 28s 2ms/step - loss: 0.3238 - Accuracy: 0.8970 - recall: 0.0016 - precision: 0.2922 - val_loss: 0.3365 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34376 to 0.33646, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 5/10\n",
      "12930/12930 [==============================] - 29s 2ms/step - loss: 0.3237 - Accuracy: 0.8971 - recall: 0.0014 - precision: 0.3148 - val_loss: 0.4076 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33646\n",
      "Epoch 6/10\n",
      "12930/12930 [==============================] - 29s 2ms/step - loss: 0.3207 - Accuracy: 0.8968 - recall: 0.0030 - precision: 0.2898 - val_loss: 0.3328 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.33646 to 0.33281, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 7/10\n",
      "12930/12930 [==============================] - 29s 2ms/step - loss: 0.3204 - Accuracy: 0.8968 - recall: 0.0031 - precision: 0.2876 - val_loss: 0.4100 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.33281\n",
      "Epoch 8/10\n",
      "12930/12930 [==============================] - 31s 2ms/step - loss: 0.3243 - Accuracy: 0.8969 - recall: 0.0029 - precision: 0.3221 - val_loss: 0.3586 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.33281\n",
      "Epoch 9/10\n",
      "12930/12930 [==============================] - 29s 2ms/step - loss: 0.3234 - Accuracy: 0.8971 - recall: 0.0034 - precision: 0.4168 - val_loss: 0.3261 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33281 to 0.32610, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 10/10\n",
      "12930/12930 [==============================] - 30s 2ms/step - loss: 0.3240 - Accuracy: 0.8972 - recall: 4.1759e-04 - precision: 0.3757 - val_loss: 0.3407 - val_Accuracy: 0.8971 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32610\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 5. Modelling -  Rohdaten\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_3\\actual_train.npz\")\n",
    "y_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_3\\actual_wsdm_labels.npz\")\n",
    "X_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_3\\actual_cv.npz\")\n",
    "y_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_3\\actual_cv_labels.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5910756, 21) (5910756,)\n",
      "(2533182, 21) (2533182,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train['arr_0']\n",
    "X_test = X_test['arr_0']\n",
    "y_train = y_train['arr_0']\n",
    "y_test = y_test['arr_0']\n",
    "\n",
    "# getting shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16840122965060518509]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting device information\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 21)]              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64)                1408      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,257\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "init = tf.keras.initializers.HeUniform()\n",
    "regular = tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(21,))\n",
    "\n",
    "# Dense hidden layer 1\n",
    "layer1 = Dense(64, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(input_layer)\n",
    "\n",
    "# Batch Normalization layer 1\n",
    "b_norm1 = BatchNormalization()(layer1)\n",
    "\n",
    "# Dropout layer 1\n",
    "drop1 = Dropout(rate=0.5)(b_norm1)\n",
    "\n",
    "# Dense hidden layer 2\n",
    "layer2 = Dense(32, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop1)\n",
    "\n",
    "# Batch Normalization layer 2\n",
    "b_norm2 = BatchNormalization()(layer2)\n",
    "\n",
    "# Dropout layer 2\n",
    "drop2 = Dropout(rate=0.5)(b_norm2)\n",
    "\n",
    "# Dense hidden layer 3\n",
    "layer3 = Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop2)\n",
    "\n",
    "# Batch Normalization layer 3\n",
    "b_norm3 = BatchNormalization()(layer3)\n",
    "\n",
    "# Dropout layer 3\n",
    "drop3 = Dropout(rate=0.5)(b_norm3)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(), kernel_regularizer=regular)(drop3)\n",
    "\n",
    "# Creating a model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "## Verwende Loss Funktion\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['Accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "cb = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23089/23089 [==============================] - 53s 2ms/step - loss: 0.2945 - Accuracy: 0.9123 - recall_6: 0.2929 - precision_6: 0.6691 - val_loss: 0.2574 - val_Accuracy: 0.9194 - val_recall_6: 0.2939 - val_precision_6: 0.7888\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25737, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 2/10\n",
      "23089/23089 [==============================] - 60s 3ms/step - loss: 0.2531 - Accuracy: 0.9145 - recall_6: 0.2864 - precision_6: 0.7112 - val_loss: 0.2440 - val_Accuracy: 0.9194 - val_recall_6: 0.3002 - val_precision_6: 0.7803\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25737 to 0.24402, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 3/10\n",
      "23089/23089 [==============================] - 57s 2ms/step - loss: 0.2533 - Accuracy: 0.9146 - recall_6: 0.2861 - precision_6: 0.7131 - val_loss: 0.2362 - val_Accuracy: 0.9197 - val_recall_6: 0.2692 - val_precision_6: 0.8417\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24402 to 0.23623, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 4/10\n",
      "23089/23089 [==============================] - 56s 2ms/step - loss: 0.2539 - Accuracy: 0.9140 - recall_6: 0.2811 - precision_6: 0.7078 - val_loss: 0.2409 - val_Accuracy: 0.9212 - val_recall_6: 0.3175 - val_precision_6: 0.7902\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.23623\n",
      "Epoch 5/10\n",
      "23089/23089 [==============================] - 56s 2ms/step - loss: 0.2543 - Accuracy: 0.9140 - recall_6: 0.2787 - precision_6: 0.7098 - val_loss: 0.2454 - val_Accuracy: 0.9214 - val_recall_6: 0.4246 - val_precision_6: 0.6907\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.23623\n",
      "Epoch 6/10\n",
      "23089/23089 [==============================] - 56s 2ms/step - loss: 0.2546 - Accuracy: 0.9139 - recall_6: 0.2759 - precision_6: 0.7109 - val_loss: 0.2432 - val_Accuracy: 0.9171 - val_recall_6: 0.2215 - val_precision_6: 0.8855\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.23623\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 6. Modelling -  Manuelles Feature Engineering (Top 10% Kaggle Leaderboard)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://machinelearningmastery.com/how-to-save-a-numpy-array-to-file-for-machine-learning/\n",
    "X_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_train.npz\")\n",
    "y_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_wsdm_labels.npz\")\n",
    "X_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_cv.npz\")\n",
    "y_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_cv_labels.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339836, 57) (339836,)\n",
      "(145644, 57) (145644,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train['arr_0']\n",
    "X_test = X_test['arr_0']\n",
    "y_train = y_train['arr_0']\n",
    "y_test = y_test['arr_0']\n",
    "\n",
    "# getting shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4121773300898880399]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting device information\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 57)]              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 64)                3712      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,785\n",
      "Trainable params: 6,561\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "init = tf.keras.initializers.HeUniform()\n",
    "regular = tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(57,))\n",
    "\n",
    "# Dense hidden layer 1\n",
    "layer1 = Dense(64, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(input_layer)\n",
    "\n",
    "# Batch Normalization layer 1\n",
    "b_norm1 = BatchNormalization()(layer1)\n",
    "\n",
    "# Dropout layer 1\n",
    "drop1 = Dropout(rate=0.5)(b_norm1)\n",
    "\n",
    "# Dense hidden layer 2\n",
    "layer2 = Dense(32, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop1)\n",
    "\n",
    "# Batch Normalization layer 2\n",
    "b_norm2 = BatchNormalization()(layer2)\n",
    "\n",
    "# Dropout layer 2\n",
    "drop2 = Dropout(rate=0.5)(b_norm2)\n",
    "\n",
    "# Dense hidden layer 3\n",
    "layer3 = Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop2)\n",
    "\n",
    "# Batch Normalization layer 3\n",
    "b_norm3 = BatchNormalization()(layer3)\n",
    "\n",
    "# Dropout layer 3\n",
    "drop3 = Dropout(rate=0.5)(b_norm3)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(), kernel_regularizer=regular)(drop3)\n",
    "\n",
    "# Creating a model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "## Verwende Loss Funktion\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['Accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "cb = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1328/1328 [==============================] - 6s 3ms/step - loss: 0.6754 - Accuracy: 0.8871 - recall_12: 0.5313 - precision_12: 0.4046 - val_loss: 0.2532 - val_Accuracy: 0.9555 - val_recall_12: 0.6320 - val_precision_12: 0.8367\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25320, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 2/10\n",
      "1328/1328 [==============================] - 3s 2ms/step - loss: 0.1847 - Accuracy: 0.9510 - recall_12: 0.6378 - precision_12: 0.7795 - val_loss: 0.3063 - val_Accuracy: 0.9004 - val_recall_12: 0.8915 - val_precision_12: 0.4738\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25320\n",
      "Epoch 3/10\n",
      "1328/1328 [==============================] - 4s 3ms/step - loss: 0.1669 - Accuracy: 0.9529 - recall_12: 0.6615 - precision_12: 0.7835 - val_loss: 0.1447 - val_Accuracy: 0.9553 - val_recall_12: 0.7242 - val_precision_12: 0.7687\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25320 to 0.14473, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 4/10\n",
      "1328/1328 [==============================] - 3s 3ms/step - loss: 0.1614 - Accuracy: 0.9529 - recall_12: 0.6577 - precision_12: 0.7855 - val_loss: 0.1607 - val_Accuracy: 0.9419 - val_recall_12: 0.3856 - val_precision_12: 0.9364\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14473\n",
      "Epoch 5/10\n",
      "1328/1328 [==============================] - 3s 2ms/step - loss: 0.1574 - Accuracy: 0.9531 - recall_12: 0.6596 - precision_12: 0.7862 - val_loss: 0.1515 - val_Accuracy: 0.9589 - val_recall_12: 0.7649 - val_precision_12: 0.7781\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14473\n",
      "Epoch 6/10\n",
      "1328/1328 [==============================] - 3s 2ms/step - loss: 0.1544 - Accuracy: 0.9535 - recall_12: 0.6547 - precision_12: 0.7942 - val_loss: 0.1355 - val_Accuracy: 0.9583 - val_recall_12: 0.6501 - val_precision_12: 0.8550\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14473 to 0.13546, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 7/10\n",
      "1328/1328 [==============================] - 3s 2ms/step - loss: 0.1544 - Accuracy: 0.9529 - recall_12: 0.6472 - precision_12: 0.7933 - val_loss: 0.2513 - val_Accuracy: 0.9354 - val_recall_12: 0.7925 - val_precision_12: 0.6107\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13546\n",
      "Epoch 8/10\n",
      "1328/1328 [==============================] - 3s 2ms/step - loss: 0.1558 - Accuracy: 0.9529 - recall_12: 0.6399 - precision_12: 0.7983 - val_loss: 0.2412 - val_Accuracy: 0.9572 - val_recall_12: 0.7044 - val_precision_12: 0.7992\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13546\n",
      "Epoch 9/10\n",
      "1328/1328 [==============================] - 3s 2ms/step - loss: 0.1545 - Accuracy: 0.9525 - recall_12: 0.6397 - precision_12: 0.7940 - val_loss: 0.1386 - val_Accuracy: 0.9580 - val_recall_12: 0.6166 - val_precision_12: 0.8853\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13546\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 7. Modelling -  Manuelles Feature Engineering (Top 10% Kaggle Leaderboard) mit Rohdaten\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://machinelearningmastery.com/how-to-save-a-numpy-array-to-file-for-machine-learning/\n",
    "X_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_train_incraw.npz\")\n",
    "y_train = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_wsdm_labels_incraw.npz\")\n",
    "X_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_cv_incraw.npz\")\n",
    "y_test = np.load(r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed\\actual_cv_labels_incraw.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5910756, 68) (5910756,)\n",
      "(2533182, 68) (2533182,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train['arr_0']\n",
    "X_test = X_test['arr_0']\n",
    "y_train = y_train['arr_0']\n",
    "y_test = y_test['arr_0']\n",
    "\n",
    "# getting shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8984606731326651300]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting device information\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 68)]              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                4416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 7,489\n",
      "Trainable params: 7,265\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Flatten, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "init = tf.keras.initializers.HeUniform()\n",
    "regular = tf.keras.regularizers.l2(l2=0.01)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(68,))\n",
    "\n",
    "# Dense hidden layer 1\n",
    "layer1 = Dense(64, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(input_layer)\n",
    "\n",
    "# Batch Normalization layer 1\n",
    "b_norm1 = BatchNormalization()(layer1)\n",
    "\n",
    "# Dropout layer 1\n",
    "drop1 = Dropout(rate=0.5)(b_norm1)\n",
    "\n",
    "# Dense hidden layer 2\n",
    "layer2 = Dense(32, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop1)\n",
    "\n",
    "# Batch Normalization layer 2\n",
    "b_norm2 = BatchNormalization()(layer2)\n",
    "\n",
    "# Dropout layer 2\n",
    "drop2 = Dropout(rate=0.5)(b_norm2)\n",
    "\n",
    "# Dense hidden layer 3\n",
    "layer3 = Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=regular)(drop2)\n",
    "\n",
    "# Batch Normalization layer 3\n",
    "b_norm3 = BatchNormalization()(layer3)\n",
    "\n",
    "# Dropout layer 3\n",
    "drop3 = Dropout(rate=0.5)(b_norm3)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(), kernel_regularizer=regular)(drop3)\n",
    "\n",
    "# Creating a model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "adam = tf.keras.optimizers.Adam()\n",
    "## Verwende Loss Funktion\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['Accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1, restore_best_weights=True)\n",
    "\n",
    "cb = [es, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23089/23089 [==============================] - 64s 3ms/step - loss: 0.2514 - Accuracy: 0.9229 - recall_11: 0.4346 - precision_11: 0.7022 - val_loss: 0.1993 - val_Accuracy: 0.9347 - val_recall_11: 0.5187 - val_precision_11: 0.7709\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19926, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 2/10\n",
      "23089/23089 [==============================] - 57s 2ms/step - loss: 0.2112 - Accuracy: 0.9260 - recall_11: 0.4540 - precision_11: 0.7236 - val_loss: 0.1829 - val_Accuracy: 0.9366 - val_recall_11: 0.6352 - val_precision_11: 0.7157\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19926 to 0.18289, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 3/10\n",
      "23089/23089 [==============================] - 57s 2ms/step - loss: 0.2107 - Accuracy: 0.9262 - recall_11: 0.4577 - precision_11: 0.7229 - val_loss: 0.1799 - val_Accuracy: 0.9371 - val_recall_11: 0.5273 - val_precision_11: 0.7905\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.18289 to 0.17991, saving model to D:\\Work_Masterarbeit\\wsdm\\preprocessed_4\\best_modelddl.h5\n",
      "Epoch 4/10\n",
      "23089/23089 [==============================] - 56s 2ms/step - loss: 0.2107 - Accuracy: 0.9261 - recall_11: 0.4578 - precision_11: 0.7219 - val_loss: 0.2545 - val_Accuracy: 0.9295 - val_recall_11: 0.4738 - val_precision_11: 0.7476\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.17991\n",
      "Epoch 5/10\n",
      "23089/23089 [==============================] - 56s 2ms/step - loss: 0.2107 - Accuracy: 0.9260 - recall_11: 0.4571 - precision_11: 0.7209 - val_loss: 0.1880 - val_Accuracy: 0.9327 - val_recall_11: 0.3961 - val_precision_11: 0.8847\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.17991\n",
      "Epoch 6/10\n",
      "23089/23089 [==============================] - 57s 2ms/step - loss: 0.2111 - Accuracy: 0.9257 - recall_11: 0.4552 - precision_11: 0.7193 - val_loss: 0.2137 - val_Accuracy: 0.9326 - val_recall_11: 0.3962 - val_precision_11: 0.8839\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.17991\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_test, y_test), verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlBJ/DHS1tE19RvmuJAd/k",
   "collapsed_sections": [],
   "name": "BASELINE_WSDM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
